{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbpOve8BqtdD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
        "import matplotlib.pyplot as plot\n",
        "# we can use the LabelEncoder to encode the gender feature\n",
        "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "# importing two different imputation methods that take into consideration all the features when predicting the missing values\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "# oversample the minority class using SMOTE\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uj812vhUq5Gu"
      },
      "outputs": [],
      "source": [
        "#load the datasets\n",
        "SR_main = pd.read_csv('/updated_tables/SR_fact_table.csv')\n",
        "country_demo = pd.read_csv('/updated_tables/country_demographics.csv')\n",
        "crime_rate = pd.read_csv('/updated_tables/crime_rate.csv')\n",
        "employment = pd.read_csv('/updated_tables/employement.csv')\n",
        "sub_abuse = pd.read_csv('/updated_tables/substance_abuse.csv')\n",
        "disease = pd.read_csv('/updated_tables/disease.csv')\n",
        "#merge data frames\n",
        "data_frames = [SR_main, country_demo, crime_rate,employment,sub_abuse]\n",
        "\n",
        "df_merge = pd.merge(SR_main, country_demo, how='left', left_on=['CountryKey'], right_on=['country_key'])\n",
        "df_merge = pd.merge(df_merge, crime_rate, how='left', left_on=['CountryKey'], right_on=['crime_key'])\n",
        "df_merge = pd.merge(df_merge, employment, how='left', left_on=['CountryKey'], right_on=['employement_key'])\n",
        "df_merge = pd.merge(df_merge, sub_abuse, how='left', left_on=['CountryKey'], right_on=['substance_key'])\n",
        "df_merge = pd.merge(df_merge, disease, how='left', left_on=['CountryKey'], right_on=['disease_key'])\n",
        "\n",
        "df_merge.head(5)\n",
        "df = df_merge[['CountryKey','sr_year','country_name','average_age','suicide_rate','unemployement_rate','pct_drug_use','pct_tobacco_use','pct_alcohol_disorder']]\n",
        "df.fillna(0,inplace=True)\n",
        "#,'pct_drug_use','pct_tobacco_use','pct_alcohol_disorder']]\n",
        "\n",
        "print(df.head(20))\n",
        "\n",
        "#scatter plot\n",
        "\n",
        "df.plot(kind = 'scatter', x = 'country_name', y = 'suicide_rate')\n",
        "df.plot(kind='hist', x = 'country_name', y = 'suicide_rate')\n",
        "df.plot(kind = 'box')\n",
        "\n",
        "#description\n",
        "print(df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28EQquwR0lq_"
      },
      "outputs": [],
      "source": [
        "# separate the features and the labels to be used in model development (2)\n",
        "data = df.drop([\"country_name\"], axis=1) \n",
        "labels = df[[\"suicide_rate\"]].copy()\n",
        "\n",
        "\n",
        "incomplete_rows = data.isnull()\n",
        "sample_incomplete_rows = data[data.isnull().any(axis=1)].head()\n",
        "print(incomplete_rows.sum())\n",
        "print(sample_incomplete_rows)\n",
        "\n",
        "# data imputation\n",
        "# given the task in predicting individuals with hepatitis C infection, select two of the most appropriate imputation strategies to fill the missing values and briefly explain why you have selected the particular strategies in a markdown cell below the current cell (3)\n",
        "imputer_simple = SimpleImputer(strategy='median')\n",
        "imputer_knn = KNNImputer(n_neighbors=5)\n",
        "imputer_iter = IterativeImputer(max_iter=10)\n",
        "\n",
        "# print the rows before and after being imputed with the two selected strategies (5)\n",
        "missing_data =[\"suicide_rate\"]\n",
        "missing_indexes = sample_incomplete_rows = data[data.isnull().any(axis=1)].index\n",
        "#Median\n",
        "train_median = data\n",
        "train_median[missing_data] = imputer_simple.fit_transform(train_median[missing_data])\n",
        "print(\"Median imputed data: \\n\",train_median.loc[train_median.index.isin(missing_indexes)])\n",
        "\n",
        "\n",
        "#KNN\n",
        "train_knn = data\n",
        "train_knn[missing_data] = imputer_knn.fit_transform(train_knn[missing_data])\n",
        "print(\"KNN imputed data: \\n\",train_knn.loc[train_knn.index.isin(missing_indexes)])\n",
        "\n",
        "# check for missing values in the training dataset and print how many rows can be identified with the missing values (1)\n",
        "\n",
        "data = train_knn\n",
        "data = df.drop([\"suicide_rate\",\"country_name\"], axis=1) \n",
        "labels = df[[\"suicide_rate\"]].copy()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UASK8t0_2fyT"
      },
      "outputs": [],
      "source": [
        "num_pipeline  = Pipeline([('imputer',KNNImputer(n_neighbors=5)),('scaler',StandardScaler())])\n",
        "num_features = list(X_train)\n",
        "\n",
        "full_pipeline = ColumnTransformer([(\"num\",num_pipeline,num_features)])\n",
        "\n",
        "prepared_data = full_pipeline.fit_transform(X_train)\n",
        "print(prepared_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WokDtufmdsSq"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "print(y_train)\n",
        "\n",
        "dtr = DecisionTreeRegressor()\n",
        "dtr.fit(prepared_data, y_train)\n",
        "\n",
        "dtr_predict = dtr.predict(prepared_data)\n",
        "\n",
        "dtr_mse = mean_squared_error(y_train, dtr_predict)\n",
        "dtr_mse = np.sqrt(dtr_mse)\n",
        "print(dtr_mse)\n",
        "\n",
        "#linear \n",
        "regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
        "regr.fit(prepared_data, y_train)\n",
        "\n",
        "print(regr.predict(prepared_data))\n",
        "\n",
        "#gradient\n",
        "reg = GradientBoostingRegressor(random_state=0)\n",
        "reg.fit(prepared_data, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "3qRZTZrtsVTM",
        "outputId": "c9a4cc49-767f-4f65-9d16-3ba245f3c161"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-192-26e5210397d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# print(f\"DTR Classification report:\\n {classification_report(y_test, prediction_DTR)}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"DTR confusion matrix:\\n {confusion_matrix(y_test,prediction_DTR)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"REG Classification report:\\n {classification_report(y_test, prediction_REG)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \"\"\"\n\u001b[0;32m--> 307\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;31m# No metrics support \"multiclass-multioutput\" format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multilabel-indicator\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: continuous is not supported"
          ]
        }
      ],
      "source": [
        "prepared_test_data = full_pipeline.fit_transform(X_test)\n",
        "\n",
        "prediction_DTR = dtr.predict(prepared_test_data)\n",
        "prediction_REG = regr.predict(prepared_test_data)\n",
        "prediction_GB = reg.predict(prepared_test_data)\n",
        "\n",
        "# print(f\"DTR Classification report:\\n {classification_report(y_test, prediction_DTR)}\")\n",
        "print(f\"DTR confusion matrix:\\n {confusion_matrix(y_test,prediction_DTR)}\")\n",
        "\n",
        "print(f\"REG Classification report:\\n {classification_report(y_test, prediction_REG)}\")\n",
        "print(f\"REG confusion matrix:\\n {confusion_matrix(y_test,prediction_REG)}\")\n",
        "\n",
        "print(f\"GB Classification report:\\n {classification_report(y_test, prediction_GB)}\")\n",
        "print(f\"GB confusion matrix:\\n {confusion_matrix(y_test,prediction_GB)}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "4142.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
